/usr/bin/python2.7 /home/genereux/PycharmProjects/signsp/create_model.py
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
---------------------------------
Run id: parkingsign-0.001-2conv-basic.model
Log directory: log/
---------------------------------
Training samples: 7452
Validation samples: 1864
--
Training Step: 1  | time: 13.578s
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 0064/7452
Training Step: 2  | total loss: 0.62363 | time: 19.252s
| Adam | epoch: 001 | loss: 0.62363 - acc: 0.4078 -- iter: 0128/7452
Training Step: 3  | total loss: 0.54051 | time: 23.945s
| Adam | epoch: 001 | loss: 0.54051 - acc: 0.7006 -- iter: 0192/7452
Training Step: 4  | total loss: 0.36405 | time: 28.804s
| Adam | epoch: 001 | loss: 0.36405 - acc: 0.7845 -- iter: 0256/7452
Training Step: 5  | total loss: 0.29392 | time: 33.515s
| Adam | epoch: 001 | loss: 0.29392 - acc: 0.8363 -- iter: 0320/7452
Training Step: 6  | total loss: 0.32567 | time: 38.047s
| Adam | epoch: 001 | loss: 0.32567 - acc: 0.8110 -- iter: 0384/7452
Training Step: 7  | total loss: 0.35456 | time: 42.817s
| Adam | epoch: 001 | loss: 0.35456 - acc: 0.7369 -- iter: 0448/7452
Training Step: 8  | total loss: 0.33757 | time: 47.919s
| Adam | epoch: 001 | loss: 0.33757 - acc: 0.7882 -- iter: 0512/7452
Training Step: 9  | total loss: 0.31380 | time: 54.629s
| Adam | epoch: 001 | loss: 0.31380 - acc: 0.7928 -- iter: 0576/7452
Training Step: 10  | total loss: 0.34280 | time: 63.433s
| Adam | epoch: 001 | loss: 0.34280 - acc: 0.7714 -- iter: 0640/7452
Training Step: 11  | total loss: 0.28009 | time: 70.236s
| Adam | epoch: 001 | loss: 0.28009 - acc: 0.8057 -- iter: 0704/7452
Training Step: 12  | total loss: 0.27458 | time: 75.142s
| Adam | epoch: 001 | loss: 0.27458 - acc: 0.7947 -- iter: 0768/7452
Training Step: 13  | total loss: 0.27779 | time: 80.516s
| Adam | epoch: 001 | loss: 0.27779 - acc: 0.7755 -- iter: 0832/7452
Training Step: 14  | total loss: 0.26071 | time: 86.268s
| Adam | epoch: 001 | loss: 0.26071 - acc: 0.8034 -- iter: 0896/7452
Training Step: 15  | total loss: 0.24319 | time: 91.332s
| Adam | epoch: 001 | loss: 0.24319 - acc: 0.8437 -- iter: 0960/7452
Training Step: 16  | total loss: 0.37708 | time: 96.257s
| Adam | epoch: 001 | loss: 0.37708 - acc: 0.8203 -- iter: 1024/7452
Training Step: 17  | total loss: 0.42876 | time: 101.047s
| Adam | epoch: 001 | loss: 0.42876 - acc: 0.7837 -- iter: 1088/7452
Training Step: 18  | total loss: 0.61404 | time: 106.284s
| Adam | epoch: 001 | loss: 0.61404 - acc: 0.7883 -- iter: 1152/7452
Training Step: 19  | total loss: 0.52145 | time: 112.726s
| Adam | epoch: 001 | loss: 0.52145 - acc: 0.7911 -- iter: 1216/7452
Training Step: 20  | total loss: 0.47794 | time: 119.237s
| Adam | epoch: 001 | loss: 0.47794 - acc: 0.7528 -- iter: 1280/7452
Training Step: 21  | total loss: 0.45536 | time: 124.164s
| Adam | epoch: 001 | loss: 0.45536 - acc: 0.7180 -- iter: 1344/7452
Training Step: 22  | total loss: 0.40595 | time: 129.136s
| Adam | epoch: 001 | loss: 0.40595 - acc: 0.7229 -- iter: 1408/7452
Training Step: 23  | total loss: 0.41372 | time: 133.716s
| Adam | epoch: 001 | loss: 0.41372 - acc: 0.7081 -- iter: 1472/7452
Training Step: 24  | total loss: 0.38165 | time: 138.237s
| Adam | epoch: 001 | loss: 0.38165 - acc: 0.7155 -- iter: 1536/7452
Training Step: 25  | total loss: 0.36751 | time: 142.765s
| Adam | epoch: 001 | loss: 0.36751 - acc: 0.7249 -- iter: 1600/7452
Training Step: 26  | total loss: 0.34532 | time: 147.534s
| Adam | epoch: 001 | loss: 0.34532 - acc: 0.7522 -- iter: 1664/7452
Training Step: 27  | total loss: 0.33044 | time: 152.123s
| Adam | epoch: 001 | loss: 0.33044 - acc: 0.7637 -- iter: 1728/7452
Training Step: 28  | total loss: 0.32669 | time: 156.588s
| Adam | epoch: 001 | loss: 0.32669 - acc: 0.7642 -- iter: 1792/7452
Training Step: 29  | total loss: 0.32994 | time: 161.007s
| Adam | epoch: 001 | loss: 0.32994 - acc: 0.7683 -- iter: 1856/7452
Training Step: 30  | total loss: 0.33388 | time: 165.494s
| Adam | epoch: 001 | loss: 0.33388 - acc: 0.7677 -- iter: 1920/7452
Training Step: 31  | total loss: 0.34598 | time: 170.178s
| Adam | epoch: 001 | loss: 0.34598 - acc: 0.7384 -- iter: 1984/7452
Training Step: 32  | total loss: 0.34879 | time: 174.758s
| Adam | epoch: 001 | loss: 0.34879 - acc: 0.7445 -- iter: 2048/7452
Training Step: 33  | total loss: 0.32888 | time: 179.256s
| Adam | epoch: 001 | loss: 0.32888 - acc: 0.7491 -- iter: 2112/7452
Training Step: 34  | total loss: 0.32599 | time: 183.707s
| Adam | epoch: 001 | loss: 0.32599 - acc: 0.7594 -- iter: 2176/7452
Training Step: 35  | total loss: 0.31902 | time: 188.219s
| Adam | epoch: 001 | loss: 0.31902 - acc: 0.7509 -- iter: 2240/7452
Training Step: 36  | total loss: 0.31162 | time: 193.018s
| Adam | epoch: 001 | loss: 0.31162 - acc: 0.7507 -- iter: 2304/7452
Training Step: 37  | total loss: 0.32037 | time: 197.681s
| Adam | epoch: 001 | loss: 0.32037 - acc: 0.7474 -- iter: 2368/7452
Training Step: 38  | total loss: 0.32694 | time: 202.150s
| Adam | epoch: 001 | loss: 0.32694 - acc: 0.7357 -- iter: 2432/7452
Training Step: 39  | total loss: 0.33276 | time: 206.735s
| Adam | epoch: 001 | loss: 0.33276 - acc: 0.7325 -- iter: 2496/7452
Training Step: 40  | total loss: 0.33165 | time: 211.185s
| Adam | epoch: 001 | loss: 0.33165 - acc: 0.7445 -- iter: 2560/7452
Training Step: 41  | total loss: 0.32041 | time: 215.666s
| Adam | epoch: 001 | loss: 0.32041 - acc: 0.7570 -- iter: 2624/7452
Training Step: 42  | total loss: 0.29951 | time: 220.230s
| Adam | epoch: 001 | loss: 0.29951 - acc: 0.7895 -- iter: 2688/7452
Training Step: 43  | total loss: 0.28366 | time: 224.726s
| Adam | epoch: 001 | loss: 0.28366 - acc: 0.7963 -- iter: 2752/7452
Training Step: 44  | total loss: 0.28358 | time: 229.268s
| Adam | epoch: 001 | loss: 0.28358 - acc: 0.7802 -- iter: 2816/7452
Training Step: 45  | total loss: 0.29890 | time: 233.848s
| Adam | epoch: 001 | loss: 0.29890 - acc: 0.7671 -- iter: 2880/7452
Training Step: 46  | total loss: 0.30351 | time: 238.439s
| Adam | epoch: 001 | loss: 0.30351 - acc: 0.7747 -- iter: 2944/7452
Training Step: 47  | total loss: 0.31998 | time: 242.977s
| Adam | epoch: 001 | loss: 0.31998 - acc: 0.7681 -- iter: 3008/7452
Training Step: 48  | total loss: 0.32300 | time: 247.656s
| Adam | epoch: 001 | loss: 0.32300 - acc: 0.7752 -- iter: 3072/7452
Training Step: 49  | total loss: 0.34216 | time: 252.214s
| Adam | epoch: 001 | loss: 0.34216 - acc: 0.7614 -- iter: 3136/7452
Training Step: 50  | total loss: 0.33756 | time: 256.659s
| Adam | epoch: 001 | loss: 0.33756 - acc: 0.7523 -- iter: 3200/7452
Training Step: 51  | total loss: 0.33570 | time: 261.169s
| Adam | epoch: 001 | loss: 0.33570 - acc: 0.7544 -- iter: 3264/7452
Training Step: 52  | total loss: 0.33942 | time: 265.747s
| Adam | epoch: 001 | loss: 0.33942 - acc: 0.7490 -- iter: 3328/7452
Training Step: 53  | total loss: 0.34200 | time: 270.383s
| Adam | epoch: 001 | loss: 0.34200 - acc: 0.7469 -- iter: 3392/7452
Training Step: 54  | total loss: 0.33779 | time: 274.950s
| Adam | epoch: 001 | loss: 0.33779 - acc: 0.7473 -- iter: 3456/7452
Training Step: 55  | total loss: 0.33606 | time: 279.438s
| Adam | epoch: 001 | loss: 0.33606 - acc: 0.7455 -- iter: 3520/7452
Training Step: 56  | total loss: 0.32643 | time: 283.961s
| Adam | epoch: 001 | loss: 0.32643 - acc: 0.7571 -- iter: 3584/7452
Training Step: 57  | total loss: 0.32539 | time: 288.608s
| Adam | epoch: 001 | loss: 0.32539 - acc: 0.7604 -- iter: 3648/7452
Training Step: 58  | total loss: 0.32380 | time: 293.073s
| Adam | epoch: 001 | loss: 0.32380 - acc: 0.7675 -- iter: 3712/7452
Training Step: 59  | total loss: 0.31946 | time: 297.701s
| Adam | epoch: 001 | loss: 0.31946 - acc: 0.7610 -- iter: 3776/7452
Training Step: 60  | total loss: 0.31496 | time: 302.280s
| Adam | epoch: 001 | loss: 0.31496 - acc: 0.7699 -- iter: 3840/7452
Training Step: 61  | total loss: 0.32025 | time: 307.013s
| Adam | epoch: 001 | loss: 0.32025 - acc: 0.7571 -- iter: 3904/7452
Training Step: 62  | total loss: 0.31718 | time: 311.490s
| Adam | epoch: 001 | loss: 0.31718 - acc: 0.7562 -- iter: 3968/7452
Training Step: 63  | total loss: 0.31015 | time: 316.047s
| Adam | epoch: 001 | loss: 0.31015 - acc: 0.7712 -- iter: 4032/7452
Training Step: 64  | total loss: 0.31385 | time: 320.526s
| Adam | epoch: 001 | loss: 0.31385 - acc: 0.7686 -- iter: 4096/7452
Training Step: 65  | total loss: 0.30393 | time: 325.114s
| Adam | epoch: 001 | loss: 0.30393 - acc: 0.7759 -- iter: 4160/7452
Training Step: 66  | total loss: 0.31714 | time: 329.580s
| Adam | epoch: 001 | loss: 0.31714 - acc: 0.7690 -- iter: 4224/7452
Training Step: 67  | total loss: 0.30906 | time: 334.182s
| Adam | epoch: 001 | loss: 0.30906 - acc: 0.7779 -- iter: 4288/7452
Training Step: 68  | total loss: 0.30679 | time: 338.736s
| Adam | epoch: 001 | loss: 0.30679 - acc: 0.7839 -- iter: 4352/7452
Training Step: 69  | total loss: 0.31416 | time: 343.204s
| Adam | epoch: 001 | loss: 0.31416 - acc: 0.7799 -- iter: 4416/7452
Training Step: 70  | total loss: 0.30926 | time: 347.732s
| Adam | epoch: 001 | loss: 0.30926 - acc: 0.7819 -- iter: 4480/7452
Training Step: 71  | total loss: 0.30690 | time: 352.239s
| Adam | epoch: 001 | loss: 0.30690 - acc: 0.7854 -- iter: 4544/7452
Training Step: 72  | total loss: 0.30743 | time: 356.743s
| Adam | epoch: 001 | loss: 0.30743 - acc: 0.7867 -- iter: 4608/7452
Training Step: 73  | total loss: 0.30596 | time: 361.298s
| Adam | epoch: 001 | loss: 0.30596 - acc: 0.7861 -- iter: 4672/7452
Training Step: 74  | total loss: 0.31265 | time: 365.787s
| Adam | epoch: 001 | loss: 0.31265 - acc: 0.7890 -- iter: 4736/7452
Training Step: 75  | total loss: 0.30580 | time: 370.311s
| Adam | epoch: 001 | loss: 0.30580 - acc: 0.7898 -- iter: 4800/7452
Training Step: 76  | total loss: 0.30593 | time: 374.891s
| Adam | epoch: 001 | loss: 0.30593 - acc: 0.7856 -- iter: 4864/7452
Training Step: 77  | total loss: 0.30938 | time: 379.416s
| Adam | epoch: 001 | loss: 0.30938 - acc: 0.7768 -- iter: 4928/7452
Training Step: 78  | total loss: 0.31045 | time: 384.002s
| Adam | epoch: 001 | loss: 0.31045 - acc: 0.7855 -- iter: 4992/7452
Training Step: 79  | total loss: 0.30742 | time: 388.550s
| Adam | epoch: 001 | loss: 0.30742 - acc: 0.7899 -- iter: 5056/7452
Training Step: 80  | total loss: 0.31139 | time: 393.162s
| Adam | epoch: 001 | loss: 0.31139 - acc: 0.7906 -- iter: 5120/7452
Training Step: 81  | total loss: 0.31369 | time: 397.636s
| Adam | epoch: 001 | loss: 0.31369 - acc: 0.7975 -- iter: 5184/7452
Training Step: 82  | total loss: 0.31365 | time: 402.220s
| Adam | epoch: 001 | loss: 0.31365 - acc: 0.8022 -- iter: 5248/7452
Training Step: 83  | total loss: 0.31316 | time: 406.679s
| Adam | epoch: 001 | loss: 0.31316 - acc: 0.8016 -- iter: 5312/7452
Training Step: 84  | total loss: 0.31203 | time: 411.173s
| Adam | epoch: 001 | loss: 0.31203 - acc: 0.8043 -- iter: 5376/7452
Training Step: 85  | total loss: 0.31248 | time: 415.776s
| Adam | epoch: 001 | loss: 0.31248 - acc: 0.8004 -- iter: 5440/7452
Training Step: 86  | total loss: 0.48490 | time: 420.314s
| Adam | epoch: 001 | loss: 0.48490 - acc: 0.7751 -- iter: 5504/7452
Training Step: 87  | total loss: 0.46370 | time: 424.990s
| Adam | epoch: 001 | loss: 0.46370 - acc: 0.7851 -- iter: 5568/7452
Training Step: 88  | total loss: 0.44536 | time: 429.515s
| Adam | epoch: 001 | loss: 0.44536 - acc: 0.7894 -- iter: 5632/7452
Training Step: 89  | total loss: 0.43296 | time: 434.091s
| Adam | epoch: 001 | loss: 0.43296 - acc: 0.7854 -- iter: 5696/7452
Training Step: 90  | total loss: 0.42994 | time: 438.532s
| Adam | epoch: 001 | loss: 0.42994 - acc: 0.7819 -- iter: 5760/7452
Training Step: 91  | total loss: 0.43148 | time: 442.980s
| Adam | epoch: 001 | loss: 0.43148 - acc: 0.7693 -- iter: 5824/7452
Training Step: 92  | total loss: 0.42741 | time: 447.493s
| Adam | epoch: 001 | loss: 0.42741 - acc: 0.7768 -- iter: 5888/7452
Training Step: 93  | total loss: 0.41601 | time: 451.983s
| Adam | epoch: 001 | loss: 0.41601 - acc: 0.7835 -- iter: 5952/7452
Training Step: 94  | total loss: 0.40195 | time: 456.556s
| Adam | epoch: 001 | loss: 0.40195 - acc: 0.7848 -- iter: 6016/7452
Training Step: 95  | total loss: 0.39560 | time: 461.264s
| Adam | epoch: 001 | loss: 0.39560 - acc: 0.7845 -- iter: 6080/7452
Training Step: 96  | total loss: 0.39104 | time: 465.920s
| Adam | epoch: 001 | loss: 0.39104 - acc: 0.7732 -- iter: 6144/7452
Training Step: 97  | total loss: 0.38409 | time: 470.397s
| Adam | epoch: 001 | loss: 0.38409 - acc: 0.7787 -- iter: 6208/7452
Training Step: 98  | total loss: 0.37172 | time: 474.925s
| Adam | epoch: 001 | loss: 0.37172 - acc: 0.7836 -- iter: 6272/7452
Training Step: 99  | total loss: 0.36801 | time: 479.390s
| Adam | epoch: 001 | loss: 0.36801 - acc: 0.7896 -- iter: 6336/7452
Training Step: 100  | total loss: 0.36480 | time: 484.012s
| Adam | epoch: 001 | loss: 0.36480 - acc: 0.7935 -- iter: 6400/7452
Training Step: 101  | total loss: 0.36311 | time: 488.555s
| Adam | epoch: 001 | loss: 0.36311 - acc: 0.7954 -- iter: 6464/7452
Training Step: 102  | total loss: 0.35650 | time: 493.277s
| Adam | epoch: 001 | loss: 0.35650 - acc: 0.8018 -- iter: 6528/7452
Training Step: 103  | total loss: 0.35123 | time: 497.790s
| Adam | epoch: 001 | loss: 0.35123 - acc: 0.8013 -- iter: 6592/7452
Training Step: 104  | total loss: 0.34864 | time: 502.277s
| Adam | epoch: 001 | loss: 0.34864 - acc: 0.7977 -- iter: 6656/7452
Training Step: 105  | total loss: 0.34507 | time: 506.930s
| Adam | epoch: 001 | loss: 0.34507 - acc: 0.7992 -- iter: 6720/7452
Training Step: 106  | total loss: 0.34192 | time: 511.678s
| Adam | epoch: 001 | loss: 0.34192 - acc: 0.7958 -- iter: 6784/7452
Training Step: 107  | total loss: 0.33354 | time: 516.166s
| Adam | epoch: 001 | loss: 0.33354 - acc: 0.7975 -- iter: 6848/7452
Training Step: 108  | total loss: 0.33238 | time: 520.674s
| Adam | epoch: 001 | loss: 0.33238 - acc: 0.7943 -- iter: 6912/7452
Training Step: 109  | total loss: 0.32779 | time: 525.387s
| Adam | epoch: 001 | loss: 0.32779 - acc: 0.8024 -- iter: 6976/7452
Training Step: 110  | total loss: 0.32891 | time: 530.124s
| Adam | epoch: 001 | loss: 0.32891 - acc: 0.7940 -- iter: 7040/7452
Training Step: 111  | total loss: 0.32867 | time: 534.676s
| Adam | epoch: 001 | loss: 0.32867 - acc: 0.7865 -- iter: 7104/7452
Training Step: 112  | total loss: 0.31900 | time: 539.469s
| Adam | epoch: 001 | loss: 0.31900 - acc: 0.7844 -- iter: 7168/7452
Training Step: 113  | total loss: 0.32647 | time: 544.149s
| Adam | epoch: 001 | loss: 0.32647 - acc: 0.7747 -- iter: 7232/7452
Training Step: 114  | total loss: 0.32053 | time: 548.725s
| Adam | epoch: 001 | loss: 0.32053 - acc: 0.7816 -- iter: 7296/7452
Training Step: 115  | total loss: 0.32273 | time: 553.574s
| Adam | epoch: 001 | loss: 0.32273 - acc: 0.7800 -- iter: 7360/7452
Training Step: 116  | total loss: 0.31974 | time: 558.361s
| Adam | epoch: 001 | loss: 0.31974 - acc: 0.7801 -- iter: 7424/7452
Training Step: 117  | total loss: 0.32620 | time: 603.196s
| Adam | epoch: 001 | loss: 0.32620 - acc: 0.7818 | val_loss: 0.53180 - val_acc: 0.8321 -- iter: 7452/7452
--
Training Step: 118  | total loss: 0.33551 | time: 1.979s
| Adam | epoch: 002 | loss: 0.33551 - acc: 0.7751 -- iter: 0064/7452
Training Step: 119  | total loss: 0.34392 | time: 13.888s
| Adam | epoch: 002 | loss: 0.34392 - acc: 0.7690 -- iter: 0128/7452
Training Step: 120  | total loss: 0.33975 | time: 19.743s
| Adam | epoch: 002 | loss: 0.33975 - acc: 0.7702 -- iter: 0192/7452
Training Step: 121  | total loss: 0.32866 | time: 24.266s
| Adam | epoch: 002 | loss: 0.32866 - acc: 0.7823 -- iter: 0256/7452
Training Step: 122  | total loss: 0.33068 | time: 29.264s
| Adam | epoch: 002 | loss: 0.33068 - acc: 0.7743 -- iter: 0320/7452
Training Step: 123  | total loss: 0.32938 | time: 33.996s
| Adam | epoch: 002 | loss: 0.32938 - acc: 0.7719 -- iter: 0384/7452
Training Step: 124  | total loss: 0.32640 | time: 38.508s
| Adam | epoch: 002 | loss: 0.32640 - acc: 0.7791 -- iter: 0448/7452
Training Step: 125  | total loss: 0.32150 | time: 43.199s
| Adam | epoch: 002 | loss: 0.32150 - acc: 0.7856 -- iter: 0512/7452
Training Step: 126  | total loss: 0.31729 | time: 48.072s
| Adam | epoch: 002 | loss: 0.31729 - acc: 0.7914 -- iter: 0576/7452
Training Step: 127  | total loss: 0.32221 | time: 53.982s
| Adam | epoch: 002 | loss: 0.32221 - acc: 0.7919 -- iter: 0640/7452
Training Step: 128  | total loss: 0.32224 | time: 58.843s
| Adam | epoch: 002 | loss: 0.32224 - acc: 0.7877 -- iter: 0704/7452
Training Step: 129  | total loss: 0.31819 | time: 63.636s
| Adam | epoch: 002 | loss: 0.31819 - acc: 0.7886 -- iter: 0768/7452
Training Step: 130  | total loss: 0.31324 | time: 68.429s
| Adam | epoch: 002 | loss: 0.31324 - acc: 0.7910 -- iter: 0832/7452
Training Step: 131  | total loss: 0.31340 | time: 73.018s
| Adam | epoch: 002 | loss: 0.31340 - acc: 0.7885 -- iter: 0896/7452
Training Step: 132  | total loss: 0.31051 | time: 77.780s
| Adam | epoch: 002 | loss: 0.31051 - acc: 0.7862 -- iter: 0960/7452
Training Step: 133  | total loss: 0.31081 | time: 82.323s
| Adam | epoch: 002 | loss: 0.31081 - acc: 0.7826 -- iter: 1024/7452
Training Step: 134  | total loss: 0.31382 | time: 86.945s
| Adam | epoch: 002 | loss: 0.31382 - acc: 0.7731 -- iter: 1088/7452
Training Step: 135  | total loss: 0.31237 | time: 91.737s
| Adam | epoch: 002 | loss: 0.31237 - acc: 0.7676 -- iter: 1152/7452
Training Step: 136  | total loss: 0.30955 | time: 96.436s
| Adam | epoch: 002 | loss: 0.30955 - acc: 0.7799 -- iter: 1216/7452
Training Step: 137  | total loss: 0.31265 | time: 101.047s
| Adam | epoch: 002 | loss: 0.31265 - acc: 0.7707 -- iter: 1280/7452
Training Step: 138  | total loss: 0.31055 | time: 105.539s
| Adam | epoch: 002 | loss: 0.31055 - acc: 0.7671 -- iter: 1344/7452
Training Step: 139  | total loss: 0.30639 | time: 110.050s
| Adam | epoch: 002 | loss: 0.30639 - acc: 0.7716 -- iter: 1408/7452
Training Step: 140  | total loss: 0.30630 | time: 114.638s
| Adam | epoch: 002 | loss: 0.30630 - acc: 0.7710 -- iter: 1472/7452
Training Step: 141  | total loss: 0.30743 | time: 119.221s
| Adam | epoch: 002 | loss: 0.30743 - acc: 0.7705 -- iter: 1536/7452
Training Step: 142  | total loss: 0.30712 | time: 123.873s
| Adam | epoch: 002 | loss: 0.30712 - acc: 0.7669 -- iter: 1600/7452
Training Step: 143  | total loss: 0.30466 | time: 128.432s
| Adam | epoch: 002 | loss: 0.30466 - acc: 0.7683 -- iter: 1664/7452
Training Step: 144  | total loss: 0.31063 | time: 133.060s
| Adam | epoch: 002 | loss: 0.31063 - acc: 0.7665 -- iter: 1728/7452
Training Step: 145  | total loss: 0.32123 | time: 137.739s
| Adam | epoch: 002 | loss: 0.32123 - acc: 0.7711 -- iter: 1792/7452
Training Step: 146  | total loss: 0.32479 | time: 142.161s
| Adam | epoch: 002 | loss: 0.32479 - acc: 0.7737 -- iter: 1856/7452
Training Step: 147  | total loss: 0.32835 | time: 146.858s
| Adam | epoch: 002 | loss: 0.32835 - acc: 0.7682 -- iter: 1920/7452
Training Step: 148  | total loss: 0.32234 | time: 151.343s
| Adam | epoch: 002 | loss: 0.32234 - acc: 0.7742 -- iter: 1984/7452
Training Step: 149  | total loss: 0.31483 | time: 155.817s
| Adam | epoch: 002 | loss: 0.31483 - acc: 0.7796 -- iter: 2048/7452
Training Step: 150  | total loss: 0.31860 | time: 160.421s
| Adam | epoch: 002 | loss: 0.31860 - acc: 0.7782 -- iter: 2112/7452
Training Step: 151  | total loss: 0.32135 | time: 164.936s
| Adam | epoch: 002 | loss: 0.32135 - acc: 0.7753 -- iter: 2176/7452
Training Step: 152  | total loss: 0.33217 | time: 169.464s
| Adam | epoch: 002 | loss: 0.33217 - acc: 0.7666 -- iter: 2240/7452
Training Step: 153  | total loss: 0.33109 | time: 174.048s
| Adam | epoch: 002 | loss: 0.33109 - acc: 0.7743 -- iter: 2304/7452
Training Step: 154  | total loss: 0.32974 | time: 178.599s
| Adam | epoch: 002 | loss: 0.32974 - acc: 0.7656 -- iter: 2368/7452
Training Step: 155  | total loss: 0.32841 | time: 183.208s
| Adam | epoch: 002 | loss: 0.32841 - acc: 0.7640 -- iter: 2432/7452
Training Step: 156  | total loss: 0.33345 | time: 187.771s
| Adam | epoch: 002 | loss: 0.33345 - acc: 0.7611 -- iter: 2496/7452
Training Step: 157  | total loss: 0.33287 | time: 192.249s
| Adam | epoch: 002 | loss: 0.33287 - acc: 0.7631 -- iter: 2560/7452
Training Step: 158  | total loss: 0.33964 | time: 196.736s
| Adam | epoch: 002 | loss: 0.33964 - acc: 0.7602 -- iter: 2624/7452
Training Step: 159  | total loss: 0.34462 | time: 201.355s
| Adam | epoch: 002 | loss: 0.34462 - acc: 0.7592 -- iter: 2688/7452
Training Step: 160  | total loss: 0.34850 | time: 205.951s
| Adam | epoch: 002 | loss: 0.34850 - acc: 0.7489 -- iter: 2752/7452
Training Step: 161  | total loss: 0.34382 | time: 210.478s
| Adam | epoch: 002 | loss: 0.34382 - acc: 0.7568 -- iter: 2816/7452
Training Step: 162  | total loss: 0.34414 | time: 215.067s
| Adam | epoch: 002 | loss: 0.34414 - acc: 0.7577 -- iter: 2880/7452
Training Step: 163  | total loss: 0.34139 | time: 219.867s
| Adam | epoch: 002 | loss: 0.34139 - acc: 0.7585 -- iter: 2944/7452
Training Step: 164  | total loss: 0.33760 | time: 224.504s
| Adam | epoch: 002 | loss: 0.33760 - acc: 0.7576 -- iter: 3008/7452
Training Step: 165  | total loss: 0.33404 | time: 228.981s
| Adam | epoch: 002 | loss: 0.33404 - acc: 0.7663 -- iter: 3072/7452
Training Step: 166  | total loss: 0.32569 | time: 233.470s
| Adam | epoch: 002 | loss: 0.32569 - acc: 0.7693 -- iter: 3136/7452
Training Step: 167  | total loss: 0.32354 | time: 238.025s
| Adam | epoch: 002 | loss: 0.32354 - acc: 0.7736 -- iter: 3200/7452
Training Step: 168  | total loss: 0.32512 | time: 242.596s
| Adam | epoch: 002 | loss: 0.32512 - acc: 0.7760 -- iter: 3264/7452
Training Step: 169  | total loss: 0.32311 | time: 247.068s
| Adam | epoch: 002 | loss: 0.32311 - acc: 0.7765 -- iter: 3328/7452
Training Step: 170  | total loss: 0.31878 | time: 251.880s
| Adam | epoch: 002 | loss: 0.31878 - acc: 0.7785 -- iter: 3392/7452
Training Step: 171  | total loss: 0.31955 | time: 256.383s
| Adam | epoch: 002 | loss: 0.31955 - acc: 0.7741 -- iter: 3456/7452
Training Step: 172  | total loss: 0.31893 | time: 260.929s
| Adam | epoch: 002 | loss: 0.31893 - acc: 0.7780 -- iter: 3520/7452
Training Step: 173  | total loss: 0.32485 | time: 265.485s
| Adam | epoch: 002 | loss: 0.32485 - acc: 0.7673 -- iter: 3584/7452
Training Step: 174  | total loss: 0.32210 | time: 270.126s
| Adam | epoch: 002 | loss: 0.32210 - acc: 0.7734 -- iter: 3648/7452
Training Step: 175  | total loss: 0.32019 | time: 274.627s
| Adam | epoch: 002 | loss: 0.32019 - acc: 0.7820 -- iter: 3712/7452
Training Step: 176  | total loss: 0.32144 | time: 279.123s
| Adam | epoch: 002 | loss: 0.32144 - acc: 0.7898 -- iter: 3776/7452
Training Step: 177  | total loss: 0.31638 | time: 283.650s
| Adam | epoch: 002 | loss: 0.31638 - acc: 0.7936 -- iter: 3840/7452
Training Step: 178  | total loss: 0.31802 | time: 288.130s
| Adam | epoch: 002 | loss: 0.31802 - acc: 0.7939 -- iter: 3904/7452
Training Step: 179  | total loss: 0.31819 | time: 292.705s
| Adam | epoch: 002 | loss: 0.31819 - acc: 0.7958 -- iter: 3968/7452
Training Step: 180  | total loss: 0.31203 | time: 297.131s
| Adam | epoch: 002 | loss: 0.31203 - acc: 0.8021 -- iter: 4032/7452
Training Step: 181  | total loss: 0.31751 | time: 301.667s
| Adam | epoch: 002 | loss: 0.31751 - acc: 0.7954 -- iter: 4096/7452
Training Step: 182  | total loss: 0.31083 | time: 306.181s
| Adam | epoch: 002 | loss: 0.31083 - acc: 0.8002 -- iter: 4160/7452
Training Step: 183  | total loss: 0.30711 | time: 310.705s
| Adam | epoch: 002 | loss: 0.30711 - acc: 0.7983 -- iter: 4224/7452
Training Step: 184  | total loss: 0.31650 | time: 315.177s
| Adam | epoch: 002 | loss: 0.31650 - acc: 0.7919 -- iter: 4288/7452
Training Step: 185  | total loss: 0.31959 | time: 319.742s
| Adam | epoch: 002 | loss: 0.31959 - acc: 0.7877 -- iter: 4352/7452
Training Step: 186  | total loss: 0.31473 | time: 324.221s
| Adam | epoch: 002 | loss: 0.31473 - acc: 0.7902 -- iter: 4416/7452
Training Step: 187  | total loss: 0.31486 | time: 328.606s
| Adam | epoch: 002 | loss: 0.31486 - acc: 0.7893 -- iter: 4480/7452
Training Step: 188  | total loss: 0.31883 | time: 333.010s
| Adam | epoch: 002 | loss: 0.31883 - acc: 0.7838 -- iter: 4544/7452
Training Step: 189  | total loss: 0.31782 | time: 337.499s
| Adam | epoch: 002 | loss: 0.31782 - acc: 0.7851 -- iter: 4608/7452
Training Step: 190  | total loss: 0.32019 | time: 341.996s
| Adam | epoch: 002 | loss: 0.32019 - acc: 0.7816 -- iter: 4672/7452
Training Step: 191  | total loss: 0.31841 | time: 346.490s
| Adam | epoch: 002 | loss: 0.31841 - acc: 0.7847 -- iter: 4736/7452
Training Step: 192  | total loss: 0.32123 | time: 350.927s
| Adam | epoch: 002 | loss: 0.32123 - acc: 0.7828 -- iter: 4800/7452
Training Step: 193  | total loss: 0.31645 | time: 355.410s
| Adam | epoch: 002 | loss: 0.31645 - acc: 0.7842 -- iter: 4864/7452
Training Step: 194  | total loss: 0.31749 | time: 359.912s
| Adam | epoch: 002 | loss: 0.31749 - acc: 0.7839 -- iter: 4928/7452
Training Step: 195  | total loss: 0.32151 | time: 364.432s
| Adam | epoch: 002 | loss: 0.32151 - acc: 0.7805 -- iter: 4992/7452
Training Step: 196  | total loss: 0.32230 | time: 368.939s
| Adam | epoch: 002 | loss: 0.32230 - acc: 0.7712 -- iter: 5056/7452
Training Step: 197  | total loss: 0.32255 | time: 373.542s
| Adam | epoch: 002 | loss: 0.32255 - acc: 0.7707 -- iter: 5120/7452
Training Step: 198  | total loss: 0.32005 | time: 377.978s
| Adam | epoch: 002 | loss: 0.32005 - acc: 0.7748 -- iter: 5184/7452
Training Step: 199  | total loss: 0.32479 | time: 382.493s
| Adam | epoch: 002 | loss: 0.32479 - acc: 0.7724 -- iter: 5248/7452
Training Step: 200  | total loss: 0.31127 | time: 387.150s
| Adam | epoch: 002 | loss: 0.31127 - acc: 0.7873 -- iter: 5312/7452
Training Step: 201  | total loss: 0.31039 | time: 391.706s
| Adam | epoch: 002 | loss: 0.31039 - acc: 0.7883 -- iter: 5376/7452
Training Step: 202  | total loss: 0.31741 | time: 396.239s
| Adam | epoch: 002 | loss: 0.31741 - acc: 0.7860 -- iter: 5440/7452
Training Step: 203  | total loss: 0.32417 | time: 400.827s
| Adam | epoch: 002 | loss: 0.32417 - acc: 0.7761 -- iter: 5504/7452
Training Step: 204  | total loss: 0.53397 | time: 405.308s
| Adam | epoch: 002 | loss: 0.53397 - acc: 0.7579 -- iter: 5568/7452
Training Step: 205  | total loss: 0.51065 | time: 409.760s
| Adam | epoch: 002 | loss: 0.51065 - acc: 0.7634 -- iter: 5632/7452
Training Step: 206  | total loss: 0.49831 | time: 414.328s
| Adam | epoch: 002 | loss: 0.49831 - acc: 0.7589 -- iter: 5696/7452
Training Step: 207  | total loss: 0.48368 | time: 418.897s
| Adam | epoch: 002 | loss: 0.48368 - acc: 0.7549 -- iter: 5760/7452
Training Step: 208  | total loss: 0.47664 | time: 423.502s
| Adam | epoch: 002 | loss: 0.47664 - acc: 0.7544 -- iter: 5824/7452
Training Step: 209  | total loss: 0.46670 | time: 428.006s
| Adam | epoch: 002 | loss: 0.46670 - acc: 0.7524 -- iter: 5888/7452
Training Step: 210  | total loss: 0.45438 | time: 432.546s
| Adam | epoch: 002 | loss: 0.45438 - acc: 0.7568 -- iter: 5952/7452
Training Step: 211  | total loss: 0.44222 | time: 437.052s
| Adam | epoch: 002 | loss: 0.44222 - acc: 0.7671 -- iter: 6016/7452
Training Step: 212  | total loss: 0.42802 | time: 441.754s
| Adam | epoch: 002 | loss: 0.42802 - acc: 0.7654 -- iter: 6080/7452
Training Step: 213  | total loss: 0.42097 | time: 446.390s
| Adam | epoch: 002 | loss: 0.42097 - acc: 0.7748 -- iter: 6144/7452
Training Step: 214  | total loss: 0.41340 | time: 450.910s
| Adam | epoch: 002 | loss: 0.41340 - acc: 0.7770 -- iter: 6208/7452
Training Step: 215  | total loss: 0.40282 | time: 455.404s
| Adam | epoch: 002 | loss: 0.40282 - acc: 0.7712 -- iter: 6272/7452
Training Step: 216  | total loss: 0.39282 | time: 459.868s
| Adam | epoch: 002 | loss: 0.39282 - acc: 0.7753 -- iter: 6336/7452
Training Step: 217  | total loss: 0.38593 | time: 464.474s
| Adam | epoch: 002 | loss: 0.38593 - acc: 0.7806 -- iter: 6400/7452
Training Step: 218  | total loss: 0.37478 | time: 468.955s
| Adam | epoch: 002 | loss: 0.37478 - acc: 0.7838 -- iter: 6464/7452
Training Step: 219  | total loss: 0.36645 | time: 473.449s
| Adam | epoch: 002 | loss: 0.36645 - acc: 0.7867 -- iter: 6528/7452
Training Step: 220  | total loss: 0.36113 | time: 477.910s
| Adam | epoch: 002 | loss: 0.36113 - acc: 0.7861 -- iter: 6592/7452
Training Step: 221  | total loss: 0.35673 | time: 482.533s
| Adam | epoch: 002 | loss: 0.35673 - acc: 0.7856 -- iter: 6656/7452
Training Step: 222  | total loss: 0.35100 | time: 486.999s
| Adam | epoch: 002 | loss: 0.35100 - acc: 0.7914 -- iter: 6720/7452
Training Step: 223  | total loss: 0.34605 | time: 491.511s
| Adam | epoch: 002 | loss: 0.34605 - acc: 0.7920 -- iter: 6784/7452
Training Step: 224  | total loss: 0.34332 | time: 496.033s
| Adam | epoch: 002 | loss: 0.34332 - acc: 0.7878 -- iter: 6848/7452
Training Step: 225  | total loss: 0.34519 | time: 500.618s
| Adam | epoch: 002 | loss: 0.34519 - acc: 0.7965 -- iter: 6912/7452
Training Step: 226  | total loss: 0.34147 | time: 505.216s
| Adam | epoch: 002 | loss: 0.34147 - acc: 0.8028 -- iter: 6976/7452
Training Step: 227  | total loss: 0.34189 | time: 509.836s
| Adam | epoch: 002 | loss: 0.34189 - acc: 0.8022 -- iter: 7040/7452
Training Step: 228  | total loss: 0.33775 | time: 514.503s
| Adam | epoch: 002 | loss: 0.33775 - acc: 0.8048 -- iter: 7104/7452
Training Step: 229  | total loss: 0.33308 | time: 519.385s
| Adam | epoch: 002 | loss: 0.33308 - acc: 0.8056 -- iter: 7168/7452
Training Step: 230  | total loss: 0.32567 | time: 524.101s
| Adam | epoch: 002 | loss: 0.32567 - acc: 0.8047 -- iter: 7232/7452
Training Step: 231  | total loss: 0.32568 | time: 528.541s
| Adam | epoch: 002 | loss: 0.32568 - acc: 0.8008 -- iter: 7296/7452
Training Step: 232  | total loss: 0.31116 | time: 533.199s
| Adam | epoch: 002 | loss: 0.31116 - acc: 0.8098 -- iter: 7360/7452
Training Step: 233  | total loss: 0.31459 | time: 538.016s
| Adam | epoch: 002 | loss: 0.31459 - acc: 0.8069 -- iter: 7424/7452
Training Step: 234  | total loss: 0.32170 | time: 585.119s
| Adam | epoch: 002 | loss: 0.32170 - acc: 0.8044 | val_loss: 0.50732 - val_acc: 0.8503 -- iter: 7452/7452
--
Training Step: 235  | total loss: 0.31661 | time: 1.993s
| Adam | epoch: 003 | loss: 0.31661 - acc: 0.8161 -- iter: 0064/7452
Training Step: 236  | total loss: 0.31905 | time: 4.155s
| Adam | epoch: 003 | loss: 0.31905 - acc: 0.8131 -- iter: 0128/7452
Training Step: 237  | total loss: 0.32108 | time: 9.171s
| Adam | epoch: 003 | loss: 0.32108 - acc: 0.8103 -- iter: 0192/7452
Training Step: 238  | total loss: 0.32213 | time: 14.047s
| Adam | epoch: 003 | loss: 0.32213 - acc: 0.8090 -- iter: 0256/7452
Training Step: 239  | total loss: 0.32267 | time: 18.778s
| Adam | epoch: 003 | loss: 0.32267 - acc: 0.8109 -- iter: 0320/7452
Training Step: 240  | total loss: 0.32805 | time: 23.618s
| Adam | epoch: 003 | loss: 0.32805 - acc: 0.7986 -- iter: 0384/7452
Training Step: 241  | total loss: 0.32204 | time: 28.992s
| Adam | epoch: 003 | loss: 0.32204 - acc: 0.8000 -- iter: 0448/7452
Training Step: 242  | total loss: 0.31995 | time: 33.460s
| Adam | epoch: 003 | loss: 0.31995 - acc: 0.8012 -- iter: 0512/7452
Training Step: 243  | total loss: 0.31306 | time: 38.141s
| Adam | epoch: 003 | loss: 0.31306 - acc: 0.8117 -- iter: 0576/7452
Training Step: 244  | total loss: 0.31285 | time: 43.226s
| Adam | epoch: 003 | loss: 0.31285 - acc: 0.8102 -- iter: 0640/7452
Training Step: 245  | total loss: 0.31232 | time: 47.730s
| Adam | epoch: 003 | loss: 0.31232 - acc: 0.8026 -- iter: 0704/7452
Training Step: 246  | total loss: 0.30877 | time: 52.246s
| Adam | epoch: 003 | loss: 0.30877 - acc: 0.8052 -- iter: 0768/7452
Training Step: 247  | total loss: 0.31217 | time: 56.977s
| Adam | epoch: 003 | loss: 0.31217 - acc: 0.7997 -- iter: 0832/7452
Training Step: 248  | total loss: 0.31240 | time: 61.684s
| Adam | epoch: 003 | loss: 0.31240 - acc: 0.7916 -- iter: 0896/7452
Training Step: 249  | total loss: 0.31598 | time: 66.152s
| Adam | epoch: 003 | loss: 0.31598 - acc: 0.7999 -- iter: 0960/7452
Training Step: 250  | total loss: 0.31335 | time: 71.590s
| Adam | epoch: 003 | loss: 0.31335 - acc: 0.8012 -- iter: 1024/7452
Training Step: 251  | total loss: 0.31635 | time: 76.691s
| Adam | epoch: 003 | loss: 0.31635 - acc: 0.7961 -- iter: 1088/7452
Training Step: 252  | total loss: 0.31828 | time: 81.551s
| Adam | epoch: 003 | loss: 0.31828 - acc: 0.8008 -- iter: 1152/7452
Training Step: 253  | total loss: 0.32152 | time: 86.220s
| Adam | epoch: 003 | loss: 0.32152 - acc: 0.8020 -- iter: 1216/7452
Training Step: 254  | total loss: 0.32346 | time: 90.720s
| Adam | epoch: 003 | loss: 0.32346 - acc: 0.8046 -- iter: 1280/7452
Training Step: 255  | total loss: 0.32175 | time: 95.598s
| Adam | epoch: 003 | loss: 0.32175 - acc: 0.8070 -- iter: 1344/7452
Training Step: 256  | total loss: 0.33020 | time: 100.088s
| Adam | epoch: 003 | loss: 0.33020 - acc: 0.8122 -- iter: 1408/7452
Training Step: 257  | total loss: 0.33005 | time: 104.813s
| Adam | epoch: 003 | loss: 0.33005 - acc: 0.8107 -- iter: 1472/7452
Training Step: 258  | total loss: 0.32534 | time: 109.272s
| Adam | epoch: 003 | loss: 0.32534 - acc: 0.8155 -- iter: 1536/7452
Training Step: 259  | total loss: 0.32112 | time: 113.720s
| Adam | epoch: 003 | loss: 0.32112 - acc: 0.8105 -- iter: 1600/7452
Training Step: 260  | total loss: 0.31674 | time: 118.184s
| Adam | epoch: 003 | loss: 0.31674 - acc: 0.8107 -- iter: 1664/7452
Training Step: 261  | total loss: 0.31997 | time: 122.829s
| Adam | epoch: 003 | loss: 0.31997 - acc: 0.8125 -- iter: 1728/7452
Training Step: 262  | total loss: 0.32063 | time: 127.305s
| Adam | epoch: 003 | loss: 0.32063 - acc: 0.8172 -- iter: 1792/7452
Training Step: 263  | total loss: 0.31789 | time: 131.827s
| Adam | epoch: 003 | loss: 0.31789 - acc: 0.8245 -- iter: 1856/7452
Training Step: 264  | total loss: 0.31790 | time: 136.347s
| Adam | epoch: 003 | loss: 0.31790 - acc: 0.8218 -- iter: 1920/7452
Training Step: 265  | total loss: 0.31614 | time: 140.869s
| Adam | epoch: 003 | loss: 0.31614 - acc: 0.8271 -- iter: 1984/7452
Training Step: 266  | total loss: 0.31682 | time: 145.447s
| Adam | epoch: 003 | loss: 0.31682 - acc: 0.8350 -- iter: 2048/7452
Training Step: 267  | total loss: 0.30929 | time: 149.968s
| Adam | epoch: 003 | loss: 0.30929 - acc: 0.8406 -- iter: 2112/7452
Training Step: 268  | total loss: 0.30421 | time: 154.569s
| Adam | epoch: 003 | loss: 0.30421 - acc: 0.8440 -- iter: 2176/7452
Training Step: 269  | total loss: 0.30215 | time: 159.159s
| Adam | epoch: 003 | loss: 0.30215 - acc: 0.8409 -- iter: 2240/7452
Training Step: 270  | total loss: 0.29957 | time: 163.792s
| Adam | epoch: 003 | loss: 0.29957 - acc: 0.8427 -- iter: 2304/7452
Training Step: 271  | total loss: 0.30120 | time: 168.278s
| Adam | epoch: 003 | loss: 0.30120 - acc: 0.8444 -- iter: 2368/7452
Training Step: 272  | total loss: 0.29832 | time: 172.902s
| Adam | epoch: 003 | loss: 0.29832 - acc: 0.8443 -- iter: 2432/7452
Training Step: 273  | total loss: 0.29337 | time: 177.601s
| Adam | epoch: 003 | loss: 0.29337 - acc: 0.8458 -- iter: 2496/7452
Training Step: 274  | total loss: 0.28846 | time: 182.118s
| Adam | epoch: 003 | loss: 0.28846 - acc: 0.8487 -- iter: 2560/7452
Training Step: 275  | total loss: 0.28710 | time: 186.596s
| Adam | epoch: 003 | loss: 0.28710 - acc: 0.8545 -- iter: 2624/7452
Training Step: 276  | total loss: 0.28666 | time: 191.216s
| Adam | epoch: 003 | loss: 0.28666 - acc: 0.8550 -- iter: 2688/7452
Training Step: 277  | total loss: 0.28557 | time: 195.643s
| Adam | epoch: 003 | loss: 0.28557 - acc: 0.8507 -- iter: 2752/7452
Training Step: 278  | total loss: 0.27546 | time: 200.053s
| Adam | epoch: 003 | loss: 0.27546 - acc: 0.8641 -- iter: 2816/7452
Training Step: 279  | total loss: 0.27542 | time: 204.925s
| Adam | epoch: 003 | loss: 0.27542 - acc: 0.8683 -- iter: 2880/7452
Training Step: 280  | total loss: 0.28200 | time: 209.506s
| Adam | epoch: 003 | loss: 0.28200 - acc: 0.8705 -- iter: 2944/7452
Training Step: 281  | total loss: 0.28473 | time: 213.951s
| Adam | epoch: 003 | loss: 0.28473 - acc: 0.8710 -- iter: 3008/7452
Training Step: 282  | total loss: 0.28327 | time: 218.428s
| Adam | epoch: 003 | loss: 0.28327 - acc: 0.8745 -- iter: 3072/7452
Training Step: 283  | total loss: 0.27932 | time: 222.949s
| Adam | epoch: 003 | loss: 0.27932 - acc: 0.8792 -- iter: 3136/7452
Training Step: 284  | total loss: 0.28219 | time: 227.581s
| Adam | epoch: 003 | loss: 0.28219 - acc: 0.8757 -- iter: 3200/7452
Training Step: 285  | total loss: 0.27859 | time: 232.124s
| Adam | epoch: 003 | loss: 0.27859 - acc: 0.8756 -- iter: 3264/7452
Training Step: 286  | total loss: 0.27553 | time: 236.565s
| Adam | epoch: 003 | loss: 0.27553 - acc: 0.8787 -- iter: 3328/7452
Training Step: 287  | total loss: 0.27362 | time: 241.055s
| Adam | epoch: 003 | loss: 0.27362 - acc: 0.8752 -- iter: 3392/7452
Training Step: 288  | total loss: 0.27699 | time: 245.511s
| Adam | epoch: 003 | loss: 0.27699 - acc: 0.8689 -- iter: 3456/7452
Training Step: 289  | total loss: 0.27176 | time: 249.996s
| Adam | epoch: 003 | loss: 0.27176 - acc: 0.8711 -- iter: 3520/7452
Training Step: 290  | total loss: 0.28445 | time: 254.481s
| Adam | epoch: 003 | loss: 0.28445 - acc: 0.8621 -- iter: 3584/7452
Training Step: 291  | total loss: 0.27300 | time: 259.044s
| Adam | epoch: 003 | loss: 0.27300 - acc: 0.8697 -- iter: 3648/7452
Training Step: 292  | total loss: 0.27790 | time: 263.641s
| Adam | epoch: 003 | loss: 0.27790 - acc: 0.8639 -- iter: 3712/7452
Training Step: 293  | total loss: 0.27758 | time: 268.081s
| Adam | epoch: 003 | loss: 0.27758 - acc: 0.8635 -- iter: 3776/7452
Training Step: 294  | total loss: 0.27574 | time: 272.541s
| Adam | epoch: 003 | loss: 0.27574 - acc: 0.8646 -- iter: 3840/7452
Training Step: 295  | total loss: 0.26734 | time: 277.157s
| Adam | epoch: 003 | loss: 0.26734 - acc: 0.8719 -- iter: 3904/7452
Training Step: 296  | total loss: 0.26866 | time: 281.902s
| Adam | epoch: 003 | loss: 0.26866 - acc: 0.8722 -- iter: 3968/7452
Training Step: 297  | total loss: 0.26752 | time: 286.471s
| Adam | epoch: 003 | loss: 0.26752 - acc: 0.8725 -- iter: 4032/7452
Training Step: 298  | total loss: 0.27277 | time: 291.068s
| Adam | epoch: 003 | loss: 0.27277 - acc: 0.8712 -- iter: 4096/7452
Training Step: 299  | total loss: 0.26261 | time: 295.570s
| Adam | epoch: 003 | loss: 0.26261 - acc: 0.8747 -- iter: 4160/7452
Training Step: 300  | total loss: 0.26038 | time: 300.040s
| Adam | epoch: 003 | loss: 0.26038 - acc: 0.8810 -- iter: 4224/7452
Training Step: 301  | total loss: 0.25274 | time: 304.586s
| Adam | epoch: 003 | loss: 0.25274 - acc: 0.8835 -- iter: 4288/7452
Training Step: 302  | total loss: 0.25083 | time: 309.818s
| Adam | epoch: 003 | loss: 0.25083 - acc: 0.8795 -- iter: 4352/7452
Training Step: 303  | total loss: 0.26379 | time: 314.261s
| Adam | epoch: 003 | loss: 0.26379 - acc: 0.8681 -- iter: 4416/7452
Training Step: 304  | total loss: 0.26806 | time: 318.935s
| Adam | epoch: 003 | loss: 0.26806 - acc: 0.8626 -- iter: 4480/7452
Training Step: 305  | total loss: 0.26262 | time: 323.578s
| Adam | epoch: 003 | loss: 0.26262 - acc: 0.8669 -- iter: 4544/7452
Training Step: 306  | total loss: 0.26309 | time: 328.177s
| Adam | epoch: 003 | loss: 0.26309 - acc: 0.8693 -- iter: 4608/7452
Training Step: 307  | total loss: 0.25169 | time: 332.788s
| Adam | epoch: 003 | loss: 0.25169 - acc: 0.8761 -- iter: 4672/7452
Training Step: 308  | total loss: 0.24704 | time: 337.267s
| Adam | epoch: 003 | loss: 0.24704 - acc: 0.8745 -- iter: 4736/7452
Training Step: 309  | total loss: 0.24870 | time: 341.694s
| Adam | epoch: 003 | loss: 0.24870 - acc: 0.8729 -- iter: 4800/7452
Training Step: 310  | total loss: 0.24714 | time: 346.340s
| Adam | epoch: 003 | loss: 0.24714 - acc: 0.8778 -- iter: 4864/7452
Training Step: 311  | total loss: 0.24834 | time: 350.797s
| Adam | epoch: 003 | loss: 0.24834 - acc: 0.8760 -- iter: 4928/7452
Training Step: 312  | total loss: 0.25735 | time: 355.331s
| Adam | epoch: 003 | loss: 0.25735 - acc: 0.8696 -- iter: 4992/7452
Training Step: 313  | total loss: 0.26772 | time: 360.081s
| Adam | epoch: 003 | loss: 0.26772 - acc: 0.8639 -- iter: 5056/7452
Training Step: 314  | total loss: 0.27671 | time: 364.766s
| Adam | epoch: 003 | loss: 0.27671 - acc: 0.8650 -- iter: 5120/7452
Training Step: 315  | total loss: 0.26498 | time: 370.117s
| Adam | epoch: 003 | loss: 0.26498 - acc: 0.8738 -- iter: 5184/7452
Training Step: 316  | total loss: 0.26319 | time: 376.570s
| Adam | epoch: 003 | loss: 0.26319 - acc: 0.8740 -- iter: 5248/7452
Training Step: 317  | total loss: 0.25968 | time: 382.723s
| Adam | epoch: 003 | loss: 0.25968 - acc: 0.8741 -- iter: 5312/7452
Training Step: 318  | total loss: 0.26590 | time: 389.171s
| Adam | epoch: 003 | loss: 0.26590 - acc: 0.8757 -- iter: 5376/7452
Training Step: 319  | total loss: 0.26325 | time: 394.144s
| Adam | epoch: 003 | loss: 0.26325 - acc: 0.8803 -- iter: 5440/7452
Training Step: 320  | total loss: 0.26892 | time: 399.106s
| Adam | epoch: 003 | loss: 0.26892 - acc: 0.8751 -- iter: 5504/7452
Training Step: 321  | total loss: 0.26166 | time: 404.091s
| Adam | epoch: 003 | loss: 0.26166 - acc: 0.8814 -- iter: 5568/7452
Training Step: 322  | total loss: 0.33043 | time: 408.642s
| Adam | epoch: 003 | loss: 0.33043 - acc: 0.8776 -- iter: 5632/7452
Training Step: 323  | total loss: 0.33181 | time: 413.112s
| Adam | epoch: 003 | loss: 0.33181 - acc: 0.8695 -- iter: 5696/7452
Training Step: 324  | total loss: 0.31890 | time: 417.603s
| Adam | epoch: 003 | loss: 0.31890 - acc: 0.8763 -- iter: 5760/7452
Training Step: 325  | total loss: 0.31232 | time: 422.029s
| Adam | epoch: 003 | loss: 0.31232 - acc: 0.8809 -- iter: 5824/7452
Training Step: 326  | total loss: 0.30289 | time: 426.526s
| Adam | epoch: 003 | loss: 0.30289 - acc: 0.8865 -- iter: 5888/7452
Training Step: 327  | total loss: 0.29674 | time: 431.073s
| Adam | epoch: 003 | loss: 0.29674 - acc: 0.8869 -- iter: 5952/7452
Training Step: 328  | total loss: 0.29143 | time: 435.687s
| Adam | epoch: 003 | loss: 0.29143 - acc: 0.8858 -- iter: 6016/7452
Training Step: 329  | total loss: 0.29088 | time: 440.183s
| Adam | epoch: 003 | loss: 0.29088 - acc: 0.8847 -- iter: 6080/7452
Training Step: 330  | total loss: 0.29037 | time: 444.638s
| Adam | epoch: 003 | loss: 0.29037 - acc: 0.8837 -- iter: 6144/7452
Training Step: 331  | total loss: 0.28377 | time: 449.138s
| Adam | epoch: 003 | loss: 0.28377 - acc: 0.8828 -- iter: 6208/7452
Training Step: 332  | total loss: 0.27668 | time: 453.743s
| Adam | epoch: 003 | loss: 0.27668 - acc: 0.8836 -- iter: 6272/7452
Training Step: 333  | total loss: 0.27291 | time: 458.431s
| Adam | epoch: 003 | loss: 0.27291 - acc: 0.8843 -- iter: 6336/7452
Training Step: 334  | total loss: 0.26449 | time: 462.881s
| Adam | epoch: 003 | loss: 0.26449 - acc: 0.8865 -- iter: 6400/7452
Training Step: 335  | total loss: 0.27175 | time: 467.413s
| Adam | epoch: 003 | loss: 0.27175 - acc: 0.8791 -- iter: 6464/7452
Training Step: 336  | total loss: 0.26457 | time: 471.927s
| Adam | epoch: 003 | loss: 0.26457 - acc: 0.8818 -- iter: 6528/7452
Training Step: 337  | total loss: 0.25523 | time: 476.379s
| Adam | epoch: 003 | loss: 0.25523 - acc: 0.8827 -- iter: 6592/7452
Training Step: 338  | total loss: 0.25139 | time: 481.071s
| Adam | epoch: 003 | loss: 0.25139 - acc: 0.8866 -- iter: 6656/7452
Training Step: 339  | total loss: 0.26066 | time: 485.800s
| Adam | epoch: 003 | loss: 0.26066 - acc: 0.8776 -- iter: 6720/7452
Training Step: 340  | total loss: 0.26037 | time: 490.309s
| Adam | epoch: 003 | loss: 0.26037 - acc: 0.8789 -- iter: 6784/7452
Training Step: 341  | total loss: 0.25261 | time: 496.446s
| Adam | epoch: 003 | loss: 0.25261 - acc: 0.8832 -- iter: 6848/7452
Training Step: 342  | total loss: 0.24681 | time: 503.870s
| Adam | epoch: 003 | loss: 0.24681 - acc: 0.8871 -- iter: 6912/7452
Training Step: 343  | total loss: 0.24848 | time: 509.348s
| Adam | epoch: 003 | loss: 0.24848 - acc: 0.8812 -- iter: 6976/7452
Training Step: 344  | total loss: 0.24582 | time: 514.133s
| Adam | epoch: 003 | loss: 0.24582 - acc: 0.8806 -- iter: 7040/7452
Training Step: 345  | total loss: 0.24972 | time: 518.944s
| Adam | epoch: 003 | loss: 0.24972 - acc: 0.8785 -- iter: 7104/7452
Training Step: 346  | total loss: 0.26804 | time: 526.215s
| Adam | epoch: 003 | loss: 0.26804 - acc: 0.8766 -- iter: 7168/7452
Training Step: 347  | total loss: 0.26417 | time: 531.798s
| Adam | epoch: 003 | loss: 0.26417 - acc: 0.8748 -- iter: 7232/7452
Training Step: 348  | total loss: 0.25739 | time: 536.622s
| Adam | epoch: 003 | loss: 0.25739 - acc: 0.8795 -- iter: 7296/7452
Training Step: 349  | total loss: 0.25623 | time: 541.251s
| Adam | epoch: 003 | loss: 0.25623 - acc: 0.8775 -- iter: 7360/7452
Training Step: 350  | total loss: 0.25570 | time: 545.766s
| Adam | epoch: 003 | loss: 0.25570 - acc: 0.8804 -- iter: 7424/7452
Training Step: 351  | total loss: 0.26325 | time: 592.335s
| Adam | epoch: 003 | loss: 0.26325 - acc: 0.8736 | val_loss: 0.28973 - val_acc: 0.9195 -- iter: 7452/7452
--

Process finished with exit code 0

